{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "c4_chap1-3_CART.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOd/haEKrjXmIbeafxlNfgh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/al025/Machine-Learning-Study-Notes/blob/master/c4_chap1_3_CART.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDfrqA8BmQ1B",
        "colab_type": "text"
      },
      "source": [
        "#### Application\n",
        "- predict whether a tumor is benign or malignant \n",
        "- predict miles per gallon(mpg) for a car \n",
        "\n",
        "#### Objectives\n",
        "- use DecisionTreeClassifier to infer class labels\n",
        "- use DecisionTreeRegressor to predict continuous target values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFx-0OySkfCs",
        "colab_type": "text"
      },
      "source": [
        "## Classification and Regression Tree (CART)\n",
        "- an internal node is split to maximize **information gain**\n",
        "- information gain is measured using *gini index* or *entropy*\n",
        "- sequence of if-else questions about individual features\n",
        "###### Advantages:\n",
        "- easy to interpret\n",
        "- no need for scaling\n",
        "###### Limitations:\n",
        "- (classifier) othogonal decision boundary \n",
        "- without constraints may lead to high variance (overfitting)\n",
        "- sensitive to small variations in the training datasets \n",
        "\n",
        "### Demo 1: use DecisionTreeClassifier to decide a tumor is benign or malignant\n",
        "\n",
        "- the **decision boundary** where examples of different classes are separated into different subspace in the feature space is linear for linear models (such as LogisticRegression, LinearSVC) but **nonlinear** & **orthogonal** for DecisionTreeClassifier\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufgUSdatkWzG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d549afe2-b869-4001-8cd0-5f22d3deaefe"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "dir = '/content/drive/My Drive/MachineLearningDatacampCareerTrack/c4_tree_based_models/data'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRfBqaZRopfU",
        "colab_type": "code",
        "outputId": "02644e7f-b4a1-4bcf-e15a-afe4459157b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# load data \n",
        "fname = dir + '/wbc.csv'\n",
        "df = pd.read_csv(fname)\n",
        "# convert str class labels to numerical values ## this is unnecessary, scikit-learn accepts categorical values as targets (although it does not accept categorical features)\n",
        "# num_label_dict = dict(zip(['M', 'B'], [1, 0])) \n",
        "# df.diagnosis = df['diagnosis'].map(num_label_dict, na_action='ignore')\n",
        "# set up feature matrix and class label vector\n",
        "X, y = df.drop('diagnosis', axis=1).values, df['diagnosis'].values\n",
        "# replace nan with the mean of this feature\n",
        "X = SimpleImputer().fit_transform(X)\n",
        "# split into 80% training set, 20% test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "parameters = {'max_depth':[3, 6, 9, 12]}\n",
        "dt = DecisionTreeClassifier()\n",
        "cv = GridSearchCV(dt, parameters, cv=5)\n",
        "cv.fit(X_train, y_train)\n",
        "y_pred = cv.predict(X_test)\n",
        "\n",
        "print(\"Tuned hyperparameters: {}\".format(cv.best_params_))\n",
        "print(\"Best cross validation score in training: {}\".format(cv.best_score_))\n",
        "print(\"Classification report in test:\\n {}\".format(classification_report(y_test, y_pred)))\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tuned hyperparameters: {'max_depth': 6}\n",
            "Best cross validation score in training: 0.9472527472527472\n",
            "Classification report in test:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           B       0.95      0.97      0.96        72\n",
            "           M       0.95      0.90      0.93        42\n",
            "\n",
            "    accuracy                           0.95       114\n",
            "   macro avg       0.95      0.94      0.94       114\n",
            "weighted avg       0.95      0.95      0.95       114\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRTRgUJAarVm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "outputId": "bda4bd5a-46b3-440f-c9c2-981bf2c88627"
      },
      "source": [
        "unconstrained_dt = DecisionTreeClassifier()\n",
        "unconstrained_dt.fit(X_train, y_train)\n",
        "y_train_pred = unconstrained_dt.predict(X_train)\n",
        "print(\"training set classification report\\n {}\".format(classification_report(y_train, y_train_pred)))\n",
        "y_pred = unconstrained_dt.predict(X_test)\n",
        "print(\"test set classification report\\n {}\".format(classification_report(y_test, y_pred)))\n",
        "print(\"depth of the decision tree: {}\".format(unconstrained_dt.get_depth()))\n",
        "print(\"number of leaves of the decision tree: {}\".format(unconstrained_dt.get_n_leaves()))\n",
        "# overfitting ???"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training set classification report\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           B       1.00      1.00      1.00       288\n",
            "           M       1.00      1.00      1.00       167\n",
            "\n",
            "    accuracy                           1.00       455\n",
            "   macro avg       1.00      1.00      1.00       455\n",
            "weighted avg       1.00      1.00      1.00       455\n",
            "\n",
            "test set classification report\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           B       0.97      0.88      0.92        69\n",
            "           M       0.84      0.96      0.90        45\n",
            "\n",
            "    accuracy                           0.91       114\n",
            "   macro avg       0.91      0.92      0.91       114\n",
            "weighted avg       0.92      0.91      0.91       114\n",
            "\n",
            "depth of the decision tree: 6\n",
            "number of leaves of the decision tree: 14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKDXW6T81jTP",
        "colab_type": "text"
      },
      "source": [
        "### Demo 2: Use DecisionTreeRegressor to predict car mpg "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dks2ewWys62u",
        "colab_type": "code",
        "outputId": "3dd63bba-3d7a-4dff-9b90-c1ae7cb498b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "import pandas as pd \n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from scipy.stats import uniform, randint\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error as MSE\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# load data\n",
        "fname = dir + '/auto.csv'\n",
        "df = pd.read_csv(fname)\n",
        "# covert categorical feature origin to numerical values\n",
        "df = pd.get_dummies(df, drop_first=True)\n",
        "X, y = df.drop('mpg', axis=1).values, df.mpg\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
        "\n",
        "# use RandomizedSearchCV to find the best hyperparameters for DecisionTreeRegressor\n",
        "params = {'max_depth': randint(1,12),\n",
        "      'min_samples_leaf': uniform(loc=0, scale=0.5)}\n",
        "dt = DecisionTreeRegressor()\n",
        "cv = RandomizedSearchCV(dt, params, cv=5)\n",
        "cv.fit(X_train, y_train)\n",
        "y_pred = cv.predict(X_test)\n",
        "# report root of mean squared error of dt \n",
        "rmse_dt = MSE(y_test, y_pred) ** 0.5\n",
        "print(\"RMSE of decision tree regressor: {:.2f}\".format(rmse_dt))\n",
        "\n",
        "# compare with a LinearRegression model\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "\n",
        "rmse_lr = MSE(y_test, y_pred_lr) ** 0.5\n",
        "print(\"RMSE of linear regreesion: {:.2f}\".format(rmse_lr))\n",
        "\n",
        "# in this example, LinearRegression outperforms DecisionTreeRegressor even if \n",
        "# lr doesn't have data scaling and hyperparameter tunning"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE of decision tree regressor: 4.45\n",
            "RMSE of linear regreesion: 4.40\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxrI25vRYO0c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "f68a7446-29cf-48ff-8a9e-547c7b895b13"
      },
      "source": [
        "unconstrained_dt = DecisionTreeRegressor()\n",
        "unconstrained_dt.fit(X_train, y_train)\n",
        "y_pred = unconstrained_dt.predict(X_train)\n",
        "rmse_undt = MSE(y_train, y_pred)**0.5\n",
        "print(\"Unconstrained dtr RMSE: {:.2f}\".format(rmse_dt))\n",
        "\n",
        "print(unconstrained_dt.tree_.node_count)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unconstrained dtr RMSE: 4.45\n",
            "537\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ae2YCgh7grXi",
        "colab_type": "text"
      },
      "source": [
        "### Chap 2 Objectives\n",
        "- understand the bias-variance tradeoff when we talk about model complexity, their relationship with underfitting and overfitting\n",
        "- use sklearn.ensemble.VotingClassifier\n",
        "\n",
        "### ensemble\n",
        "- different models trained on the same dataset\n",
        "- let each model make their own prediction\n",
        "- final prediction of the ensemble aggregates individual predictions, (hopefully) is more robust & less prone to errors\n",
        "\n",
        "#### demo 3: ensembles outperforms all its component individual classifiers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRLzkczgg0lK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "406be37c-21e3-465f-81e8-eea28ca2897b"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# load data\n",
        "fname = dir + '/indian_liver_patient/indian_liver_patient_preprocessed.csv'\n",
        "df = pd.read_csv(fname)\n",
        "X, y = df.drop('Liver_disease', axis=1), df.Liver_disease\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7, stratify=y)\n",
        "\n",
        "# initiate each individual classifier \n",
        "dt = DecisionTreeClassifier(min_samples_leaf=0.13, random_state=7)\n",
        "knn = KNeighborsClassifier(n_neighbors=27)\n",
        "lr = LogisticRegression(random_state=7)\n",
        "\n",
        "classifiers = [('decision tree', dt), ('KNN', knn), ('logistic regression', lr)]\n",
        "for name, clf in classifiers:\n",
        "  clf.fit(X_train, y_train)\n",
        "  print(\"{:s} {:.3f}\".format(name, clf.score(X_test, y_test)))\n",
        "\n",
        "vc = VotingClassifier(estimators=classifiers)\n",
        "vc.fit(X_train, y_train)\n",
        "print(\"voting classifier {:.3f}\".format(vc.score(X_test, y_test)))\n",
        "\n",
        "### in my experiments, the ensemble did not outperform all individual classifiers every time"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "decision tree 0.690\n",
            "KNN 0.713\n",
            "logistic regression 0.672\n",
            "voting classifier 0.678\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7IhPLoWsYqz",
        "colab_type": "text"
      },
      "source": [
        "## bagging\n",
        "- one model, different subsets of training data (sampled with replacement from the training set)\n",
        "- **bootstrap** aggregation\n",
        "- on average, 63% of the training sets are sampled \n",
        "- the remaining called **out-of-bag (OOB)** samples can be used to evaluated the model's generalization error. This is known as OOB evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGO3rydIsXrR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "8eb58fc0-69ca-4bdb-c0cb-e1a92195e082"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "# load data \n",
        "fname = dir + '/wbc.csv'\n",
        "df = pd.read_csv(fname)\n",
        "# set up feature matrix and class label vector\n",
        "X, y = df.drop('diagnosis', axis=1).values, df['diagnosis'].values\n",
        "# replace nan with the mean of this feature\n",
        "X = SimpleImputer().fit_transform(X)\n",
        "# split into 80% training set, 20% test set\n",
        "SEED = 1\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED, stratify=y)\n",
        "\n",
        "dt = DecisionTreeClassifier(min_samples_leaf=8, random_state=SEED)\n",
        "bc = BaggingClassifier(base_estimator=dt, n_estimators=50, oob_score=True, random_state=SEED)\n",
        "\n",
        "bc.fit(X_train, y_train)\n",
        "print(\"bagging classifier test set accuracy: {:.3f}\".format(bc.score(X_test, y_test)))\n",
        "print('bagging classifier oob score: {:.3f}'.format(bc.oob_score_))\n",
        "### bagging classifier does not outperform an individual decision tree classifier why???\n",
        "### oob score is not close to test accuracy when random_state =2, 5, 7... why??? "
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bagging classifier test set accuracy: 0.942\n",
            "bagging classifier oob score: 0.947\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}